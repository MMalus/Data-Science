{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Exploring Twitter data for the first round of the French Presidential Election",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#start by importing the necessary libraries to play with the data\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#first we need to load the sqlite files into a pandas dataframe\n#remember to open and close the sqlite connection, and that we have multiple databases\n\n#first create a list to help with importing the files\n\nnumlist = [str(i)+\"_\"+str(j) for i in range(11,18) for j in range(0,2)]\n\n#remove ones that aren't there, add google grouping\n\nnumlist.remove('11_0')\nnumlist.remove('17_1')\n#numlist.append('googletrends') deal with google trends later\n\n#create a list to house the giant collection of dfs\n\ndf_list = []\n\nfor x in numlist:\n    base = \"../input/database_\"\n    path = base + x + \".sqlite\"\n    connection = sqlite3.connect(path)\n    df_list.append(pd.read_sql_query(\"SELECT * from data\", connection))\n    connection.close()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now the list collection is enormous, but since all the dataframes have the same columns in them, I'm just gonna grab one to use as an example for the rest of the exploration.  Specifically, the last one.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#just pick the last element of the list, and let's start doing some EDA\n\ndf = df_list[-1]\n\n#start with columns, to get an idea of what variables we could have to work with\ndf.columns",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Right off the bat, the most interesting looking column to me is language.  Knee jerk reaction may be to toss any tweets that aren't in French, but let's see if locations match in a reasonable way.  In other words, if there are Spanish tweets in the Southwest of France about the election, maybe we shouldn't be so quick to drop them.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#let's start by taking a look at what regions there are, and what languages\n\ndf['lang'].value_counts()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['location'].value_counts()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This is quite a mess.  There's a jumble of single use locations (including Laos), and a mess of languages as well.  To deal with the languages, we'll use Wikipedia to pick only the top five languages spoken in France.  According to wikipedia, we'll want: French, English, Spanish, German, and Italian.  There's also another category \"und\" which ranks high up.  We'll nab it too.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#we redo the df to only use those languages\nlanglist = ['fr', 'en', 'und', 'de', 'it', 'es']\ndf = df[df['lang'].isin(langlist)]\ndf.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now our dataframe is a bit more reasonable.  There's still some improvement to be done, specifically on the location column.  We want to thin out the herd a little, too many entries with just 1 hit.  Finding a good cutoff point is really difficult though.  But I'm just going to use this an opportunity to demonstrate some techniques.  Here's what we plan to do:\n\n1) Turn entries like \"FRANCE\", \"France\", and \"france\" into \"France\"\n\n2) Turn entries like \"Amiens, France\" into \"Amiens\"\n\n3) Drop obviously outside France entries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#start with number 1\n\n#we'll make a list of the offending names\nfrancelist = ['FRANCE', 'france', 'France', 'France ']\n\n#now simply use the df.replace function from pandas and we're good to go\ndf['location'] = df['location'].replace(to_replace=francelist, value='France')\n\n#just a quick check to make sure thinks look okay\ndf['location'].value_counts()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#now for goal number 2, my gut instinct is to just use the str.replace function\ndf['location'] = df['location'].str.replace(', France', '')\ndf['location'].value_counts()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now number three is the toughest.  There's a variety of approaches that can be used here, but the most reasonable one would be to generate a large list of all cities, towns, villages (apparently these fall under the collective name of communes in France) in France and former colonies which can vote.  Some googling shows that there's a module called 'geography' which might work.  Otherwise, this is an enormous task.  Unfortunately, geography is not available to import on Kaggle.  Since that's the case, we'll do some other little fixes and finish up by looking at the five major players (and more specifically the two that are advancing).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#paris also suffers from a similar problem to France, let's fix that\n#we'll make a list of the offending names\nparislist = ['PARIS', 'paris', 'Paris', 'Paris ']\n\n#now simply use the df.replace function from pandas and we're good to go\ndf['location'] = df['location'].replace(to_replace=parislist, value='Paris')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#a decision we can make to deal with some of the trashy locations is to just cull all locations\n#that only appear, say, less than 100 times.\n\n#it seems Kaggle can't handle this, but here's how it would be done\n\n#thresholdnum = 100\n#value_counts = df['location'].value_counts()\n#to_remove = value_counts[value_counts <= thresholdnum].index\n#df['location'].replace(to_remove, np.nan, inplace=True)\n\n#df['location'].value_counts()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}